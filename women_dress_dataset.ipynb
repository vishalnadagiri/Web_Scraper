{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4  import BeautifulSoup\n",
    "import requests\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\User\\Downloads\\Compressed\\chromedriver_win32_2\\chromedriver.exe\",options=chrome_options)\n",
    "\n",
    "srch_url=\"https://www.flipkart.com/womens-dresses/pr?sid=clo%2Codx%2Cmaj%2Cjhy&marketplace=FLIPKART&otracker=product_breadCrumbs_Women%27s+Dresses\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https www flipkart c'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('\\W+',' ',srch_url )[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_img(url,scd):\n",
    "    img_data = requests.get(url).content\n",
    "    if not os.path.exists('downloaded_image'):\n",
    "        os.makedirs('downloaded_image')\n",
    "    else:\n",
    "        img_name=os.getcwd()+'/downloaded_image'+'/%0.f_SC_%s.jpg'%(time.time(),re.sub('\\W+',' ',scd )[:20])\n",
    "        with open(img_name, 'wb') as handler:\n",
    "            handler.write(img_data)\n",
    "#             print(img_name+'  Image downloaded')\n",
    "        return './downloaded_image'+'/%0.f_SC_%s.jpg'%(time.time(),re.sub('\\W+',' ',scd ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scrape_one_prod:\n",
    "#     global otdic\n",
    "    def __init__(self):\n",
    "        self.name= list()\n",
    "        self.price = list()\n",
    "        self.dis = list()\n",
    "        self.url = []\n",
    "        self.df_p= pd.DataFrame()\n",
    "        \n",
    "    def scrape_details(self,srch_url,pg_no):\n",
    "        driver.get(srch_url+r'&page='+str(pg_no))\n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content)\n",
    "        error_url = list()\n",
    "        for i,a in enumerate(tqdm.tqdm(soup.findAll(attrs={'class':'_2B099V'}))):\n",
    "            \n",
    "            try: \n",
    "                a.find('a',attrs='IRpwTa')['href']\n",
    "                self.url='https://www.flipkart.com' + a.find('a',attrs='IRpwTa')['href']\n",
    "#                 url.append(+self.url)\n",
    "                self.scrape_product(self.url,a)\n",
    "            except TypeError:\n",
    "                error_url.append('https://www.flipkart.com' + a.find('a',attrs='IRpwTa')['href'])\n",
    "                print('Error in fetching product trying to fetch next product in 5sec')\n",
    "                time.sleep(5)\n",
    "                \n",
    "        return error_url\n",
    "    \n",
    "    def scrape_product(self,url_,a):\n",
    "#         print(url_)\n",
    "        driver.get(url_)\n",
    "        content_=driver.page_source\n",
    "        soup = BeautifulSoup(content_)\n",
    "        otdic = dict()\n",
    "        dec,colr_,col,siz = str(),str(),list(),list()\n",
    "        \n",
    "        #Name Product color rating and product price\n",
    "        self.name=a.find('a',attrs='IRpwTa')\n",
    "        self.dis=a.find('div',attrs='_3Ay6Sb')\n",
    "        self.price=a.find('div',attrs='_30jeq3')\n",
    "        rat = str()\n",
    "        if soup.find(attrs={'class':'_3LWZlK _3uSWvT'}): #soup.find(attrs={'class':'_3LWZlK _3uSWvT'}).text\n",
    "            rat = soup.find(attrs={'class':'_3LWZlK _3uSWvT'}).text\n",
    "#             print(rat)\n",
    "        else:\n",
    "            rat =None\n",
    "        \n",
    "        otdic.update({'Product_name':self.name,'Price':self.price,'Didcount':self.dis,'P_URL':url_,'Rating':rat})\n",
    "        \n",
    "        #Description\n",
    "        if soup.find('div',attrs='_1AN87F'):\n",
    "            dec = soup.find('div',attrs='_1AN87F').text\n",
    "#             otdic.update({'Description':dec})\n",
    "#             print('Product Decription found :'+otdic['Description']\n",
    "        else:\n",
    "            dec = None\n",
    "#             otdic.update({'Description':'None'})\n",
    "            \n",
    "        #Product Color    \n",
    "        if soup.find('li',attrs={'id':re.compile('-color')}):\n",
    "            for itm in soup.find_all('li','_3V2wfe _31hAvz'):\n",
    "                if itm.find('a','kmlXmn PP89tw'):\n",
    "                    colr_ = itm.find('div','_3Oikkn _3_ezix _2KarXJ _31hAvz').text\n",
    "#                     otdic.update({'P_Color':colr_})\n",
    "#                     print('Product Color :'+otdic['P_Color'])\n",
    "        else:\n",
    "            colr_=None\n",
    "#             otdic.update({'P_Color':'None'})\n",
    "            \n",
    "         \n",
    "        #Available colors\n",
    "        if soup.find('li',{'id':re.compile('-color')}):\n",
    "            for items in soup.find_all('li',{'class':'_3V2wfe _31hAvz','id':re.compile('-color')}):\n",
    "                col.append(items.find('div','_3Oikkn _3_ezix _2KarXJ _31hAvz').text)\n",
    "            #     print(items.find('div','_3Oikkn _3_ezix _2KarXJ _31hAvz').text)\n",
    "#             otdic.update({'Avl_Color':col})\n",
    "#             print('Available COlors ',end='\\t')\n",
    "#             print(otdic['Avl_Color'])\n",
    "        else:\n",
    "            col =[None]\n",
    "#             otdic.update({'Avl_Color':'None'})\n",
    "        \n",
    "        #Available sizes\n",
    "        if soup.find('li',{'id':re.compile('-size')}):\n",
    "            for items in soup.find_all('li',{'class':'_3V2wfe _31hAvz','id':re.compile('-size')}):\n",
    "                siz.append(items.find('div',attrs={'class':re.compile('_3Oikkn _3_ezix _2KarXJ _31hAvz')}).text)\n",
    "            #     print(items.find('div','_3Oikkn _3_ezix _2KarXJ _31hAvz').text)            \n",
    "#             otdic.update({'Avl_Size':siz})\n",
    "#             print('Available Size ',end='\\t')\n",
    "#             print(otdic['Avl_Size']) \n",
    "        else:\n",
    "            siz = [None]\n",
    "#             otdic.update({'Avl_Size':'None'})\n",
    "        \n",
    "        #Image URL\n",
    "        img_url_ = soup.find('img','_2r_T1I _396QI4')['src']\n",
    "        \n",
    "#         Other details of product\n",
    "        ot_ky, ky_vl = [],[]\n",
    "        for k,v in zip(soup.find_all('div',attrs=['_2H87wv',]), soup.find_all('div',attrs=['_2vZqPX'])):\n",
    "#             print(k.text+': '+v.text)\n",
    "#             otdic.update({k.text:v.text})\n",
    "            ot_ky.append(k.text)\n",
    "            ky_vl.append(v.text)\n",
    "        otdic.update({k__:v__ for k__, v__ in zip(ot_ky,ky_vl)})\n",
    "#         print(otdic)\n",
    "        im_path = down_img(img_url_,otdic['Style Code'])\n",
    "        otdic.update({'Description': dec, 'P_Color':colr_ , 'Avl_Color': [col], 'Avl_Size':[siz] ,'Image_path':im_path})\n",
    "        self.df_p = pd.DataFrame.from_dict(otdic)\n",
    "        global df\n",
    "        df = pd.concat([df,self.df_p])\n",
    "#         return otdic\n",
    "#         pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "global df\n",
    "df = pd.DataFrame()\n",
    "scrapper = scrape_one_prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:49<00:00,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1 Page products and total 40 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▏                                | 24/40 [00:55<00:38,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in fetching product trying to fetch next product in 5sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:47<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 2 Page products and total 79 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:53<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 3 Page products and total 119 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:37<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 4 Page products and total 159 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [02:04<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 5 Page products and total 199 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████████████████████████▎                              | 25/40 [01:25<00:45,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in fetching product trying to fetch next product in 5sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [02:19<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 6 Page products and total 238 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:34<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 7 Page products and total 278 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:54<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 8 Page products and total 318 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:46<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 9 Page products and total 358 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|███████████████████████████████████████████████████████████████████████████████▉  | 39/40 [01:30<00:02,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in fetching product trying to fetch next product in 5sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:47<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 10 Page products and total 397 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:33<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 11 Page products and total 437 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:52<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 12 Page products and total 477 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:43<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 13 Page products and total 517 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:40<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 14 Page products and total 557 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:54<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 15 Page products and total 597 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:53<00:00,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 16 Page products and total 637 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [02:28<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 17 Page products and total 677 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [02:52<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 18 Page products and total 717 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [02:19<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 19 Page products and total 757 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [02:36<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 20 Page products and total 797 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [02:57<00:00,  4.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 21 Page products and total 837 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [02:25<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 22 Page products and total 877 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [02:40<00:00,  4.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 23 Page products and total 917 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [02:38<00:00,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 24 Page products and total 957 products \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pg_no = 25\n",
    "\n",
    "for i in range(1,pg_no):\n",
    "    error_url_list = scrapper.scrape_details(srch_url,i)\n",
    "#     print(srch_url+str(i))\n",
    "#     dic = {'Product Name':name,'Price':price,'Rating':dis,'url':url}\n",
    "#     dic.update\n",
    "#     df = df.append(pd.DataFrame(ot_dic),ignore_index=True)\n",
    "    print('Extracted %d Page products and total %d products '%(i,df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('extracted_%0.f.csv'%time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dupli(col,valu):\n",
    "    vc = df[col].value_counts()\n",
    "    return len(vc> valu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "664"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dupli('P_URL',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc  = df['Image_path'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vc[vc > 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
